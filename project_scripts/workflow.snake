#######################################
#
# Snakemake workflow analyze genomes
#
# How to run:
#
# conda create -n cfDNA_project -y snakemake snakemake-executor-plugin-slurm numpy
# conda activate cfDNA_project
# snakemake -s workflow_project.snake --executor slurm -j 20 --use-conda --default-resources slurm_account=Fragmentomics --group-components sort_dhs=50 prep_dhs=50 sort_frag=50 prep_frag=50 lwps=50 fdi=50 ifs=50 pfe=50 ocf=50
#
#######################################
import glob
import yaml

with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

INPUT_DIR = config['input_dir']
INPUT_DHS_DIR = config['input_dhs_dir']
RESULT_DIR = config['result_dir']
RESULT_SORTED_DIR = config['result_sorted_dir']
MATRIX_COLUMNS = config['matrix_columns']
EXAMPLE_DHS = config['example_dhs']
EXAMPLE_SAMPLE = config['example_sample']
RESULT_PCA_DIR = config['result_pca_dir']

SAMPLES = [f.split('/')[-1].replace('.hg38.frag.gz', '') for f in glob.glob(f"{INPUT_DIR}*.hg38.frag.gz", recursive=True)]
DHS_FILES = [f.split('/')[-1].replace('.bed', '') for f in glob.glob(f"{INPUT_DHS_DIR}*.bed", recursive=True)]
STATS = ['pfe', 'lwps', 'ifs', 'fdi', 'ocf']
STATS_WITHOUT_PFE = [s for s in STATS if s != 'pfe']

rule all:
    input:
        # final results
        expand(f"{RESULT_DIR}feature_matrix_{{stat}}.parquet", stat=STATS),
        expand(f"{RESULT_DIR}{{stat}}_pca_loadings.csv", stat=STATS_WITHOUT_PFE),
        f"{RESULT_PCA_DIR}fdi_pc1_loadings.png",
        f"{RESULT_PCA_DIR}fdi_{EXAMPLE_DHS}_pca_by_disease.png",
        f"{RESULT_PCA_DIR}pfe_{EXAMPLE_DHS}_violin_disease.png",

        
rule check_dhs_data_sorting:
    input:
        dhs=f"{INPUT_DHS_DIR}{{dhs_file}}.bed"
    output:
        dhs_sorted=f"{RESULT_SORTED_DIR}{{dhs_file}}_sorted.bed"
    resources:
        runtime=5,
        mem_mb=100
    group: "sort_dhs"
    shell:
        '''
        if cat {input.dhs} | awk -F'\t' '{{if(NR>1 && prev > $2 && $1 == prev_chr) {{print "PROBLEM"; exit 1}}; prev=$2; prev_chr=$1}}'; then
            # the file is sorted, only create the symlink
            ln -s $(realpath {input.dhs}) {output.dhs_sorted}
        else
            # the file isn't sorted, so we need to sort it by chr (with -V) and start position (with -n) and then gzip back
            sort -k1,1V -k2,2n {input.dhs} > {output.dhs_sorted}
        fi
        '''
        
rule preprocess_dhs_sorted_data:
    input:
        dhs_sorted=f"{RESULT_SORTED_DIR}{{dhs_file}}_sorted.bed"
    output:
        dhs_sorted_preprocessed=f"{RESULT_SORTED_DIR}{{dhs_file}}_sorted_wl{MATRIX_COLUMNS}.bed"
    resources:
        runtime=30,
        mem_mb=300
    group: "prep_dhs"
    script:
        "preprocess_dhs.py"
        
rule check_fragments_data_sorting:
    input:
        fragment=f"{INPUT_DIR}{{sample}}.hg38.frag.gz",
    output:
        fragment_sorted=f"{RESULT_SORTED_DIR}{{sample}}_sorted.hg38.frag.gz"
    resources:
        runtime=5,
        mem_mb=100
    group: "sort_frag"
    shell:
        '''
        if zcat {input.fragment} | awk -F'\t' '{{if(NR>1 && prev > $2 && $1 == prev_chr) {{print "PROBLEM"; exit 1}}; prev=$2; prev_chr=$1}}'; then
            # the file is sorted, only create the symlink
            ln -s $(realpath {input.fragment}) {output.fragment_sorted}
        else
            # the file isn't sorted, so we need to sort it by chr (with -V) and start position (with -n) and then gzip back
            zcat {input.fragment} | sort -k1,1V -k2,2n | gzip -c > {output.fragment_sorted}
        fi
        '''

rule preprocess_fragments:
    input:
        fragment=f"{RESULT_SORTED_DIR}{{sample}}_sorted.hg38.frag.gz",
        #dhs=f"{RESULT_SORTED_DIR}{{dhs_file}}_sorted.bed"
        dhs=f"{RESULT_SORTED_DIR}{{dhs_file}}_sorted_wl{MATRIX_COLUMNS}.bed"
    output:
        f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted.npy"
    resources:
        runtime=30,
        mem_mb=100
    group: "prep_frag"
    script:
        "preprocess_fragments.py"
        
# LWPS calculation
rule calculate_lwps:
    input:
        matrix=f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted.npy",
        config="config.yaml"
    output:
        f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_lwps.npy"
    params:
        statistic="lwps"
    resources:
        mem_mb=100,
        runtime=30
    group: "lwps"
    script:
        "calculate_statistics.py"

# FDI calculation
rule calculate_fdi:
    input:
        matrix=f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted.npy",
        config="config.yaml"
    output:
        f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_fdi.npz"
    params:
        statistic="fdi"
    resources:
        mem_mb=300,
        runtime=30
    group: "fdi"
    script:
        "calculate_statistics.py"
        
# IFS calculation
rule calculate_ifs:
    input:
        matrix=f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted.npy",
        config="config.yaml"
    output:
        f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_ifs.npz"
    params:
        statistic="ifs"
    resources:
        mem_mb=60,
        runtime=30
    group: "ifs"
    script:
        "calculate_statistics.py"

# PFE calculation
rule calculate_pfe:
    input:
        matrix=f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted.npy",
        config="config.yaml"
    output:
        f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_pfe.npz"
    params:
        statistic="pfe"
    resources:
        mem_mb=100,
        runtime=30
    group: "pfe"
    script:
        "calculate_statistics.py"
        
# OCF calculation
rule calculate_ocf:
    input:
        matrix=f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted.npy",
        config="config.yaml"
    output:
        f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_ocf.npy"
    params:
        statistic="ocf"
    resources:
        mem_mb=160,
        runtime=30
    group: "ocf"
    script:
        "calculate_statistics.py"

# merge same test statistics 
rule build_feature_matrices:
    input:
        lwps_inputs=expand(f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_lwps.npy",
                           sample=SAMPLES, dhs_file=DHS_FILES),
        ocf_inputs=expand(f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_ocf.npy",
                          sample=SAMPLES, dhs_file=DHS_FILES),
        fdi_inputs=expand(f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_fdi.npz",
                          sample=SAMPLES, dhs_file=DHS_FILES),
        ifs_inputs=expand(f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_ifs.npz",
                          sample=SAMPLES, dhs_file=DHS_FILES),
        pfe_inputs=expand(f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_pfe.npz",
                          sample=SAMPLES, dhs_file=DHS_FILES),      
        config="config.yaml",
    params:
        dhs_files=DHS_FILES,
    output:
        matrices=expand(f"{RESULT_DIR}feature_matrix_{{stat}}.parquet", stat=STATS),
        loadings=expand(f"{RESULT_DIR}{{stat}}_pca_loadings.csv", stat=STATS_WITHOUT_PFE),
    resources:
        runtime=30,
        mem_mb=8000
    script:
        "build_matrices_joint.py"
        
# visualize merged scores by DHS
rule pca_plots:
    input:
        matrices=expand(f"{RESULT_DIR}feature_matrix_{{stat}}.parquet", stat=STATS),
        loadings=expand(f"{RESULT_DIR}{{stat}}_pca_loadings.csv", stat=STATS_WITHOUT_PFE),
        config="config.yaml",
    output:
        f"{RESULT_PCA_DIR}fdi_pc1_loadings.png",
        f"{RESULT_PCA_DIR}fdi_{EXAMPLE_DHS}_pca_by_disease.png",
        f"{RESULT_PCA_DIR}pfe_{EXAMPLE_DHS}_violin_disease.png",
    params:
        stats=STATS
    resources:
        runtime=30,
        mem_mb=8000
    script:
        "pca_joint.py"