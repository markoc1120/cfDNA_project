{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "STATS = {\n",
    "    'ocf': ('{sid}__{dhs}_sorted_ocf.npy', None),\n",
    "    'lwps': ('{sid}__{dhs}_sorted_lwps.npy', None),\n",
    "    'ifs': ('{sid}__{dhs}_sorted_ifs.npz', 'ifs_scores'),\n",
    "    'pfe': ('{sid}__{dhs}_sorted_pfe.npz', 'pfe_scores'),\n",
    "    'fdi': ('{sid}__{dhs}_sorted_fdi.npz', 'overlapping_fdi_scores'),\n",
    "}\n",
    "\n",
    "\n",
    "def parse_metadata(file_path: str, paper: str) -> dict:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[df.publication == paper]\n",
    "    return dict(zip(df.sample_file_id, df.sample_disease))\n",
    "\n",
    "\n",
    "def load_vectors(stat_name: str, metadata_map: dict, data_dir: str, dhs_files):\n",
    "    pattern, key = STATS[stat_name]\n",
    "\n",
    "    entries = []\n",
    "    for sid, disease in metadata_map.items():\n",
    "        binary_label = 'Healthy' if disease == 'Healthy' else 'Cancerous'\n",
    "        for dhs in dhs_files:\n",
    "            fname = pattern.format(sid=sid, dhs=dhs)\n",
    "            path = os.path.join(data_dir, fname)\n",
    "            \n",
    "            # skip if preprocessing marked this pair as low coverage\n",
    "            matrix_base = os.path.join(data_dir, f'{sid}__{dhs}_sorted.npy')\n",
    "            if os.path.exists(matrix_base + '.skip'):\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                if path.endswith('.npy'):\n",
    "                    vec = np.load(path)\n",
    "                elif path.endswith('.npz'):\n",
    "                    data = np.load(path)\n",
    "                    vec = data[key]\n",
    "                else:\n",
    "                    continue\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "            flat = vec.flatten()\n",
    "\n",
    "            entries.append({\n",
    "                \"sample\": sid,\n",
    "                \"dhs\": dhs,\n",
    "                \"vector\": flat,\n",
    "                \"disease\": disease,\n",
    "                \"binary\": binary_label,\n",
    "            })\n",
    "\n",
    "    if not entries:\n",
    "        return None\n",
    "\n",
    "#     all_vectors = StandardScaler().fit_transform(np.vstack([e['vector'] for e in entries]))\n",
    "    all_vectors = np.vstack([e['vector'] for e in entries])\n",
    "    \n",
    "    loadings_df = None\n",
    "    if stat != 'pfe':\n",
    "        pca = PCA(n_components=2)\n",
    "        pc_values = pca.fit_transform(all_vectors)\n",
    "        expl_var = pca.explained_variance_ratio_\n",
    "        loadings = pca.components_\n",
    "        loadings_df = pd.DataFrame(\n",
    "            loadings.T,\n",
    "            columns=['PC1', 'PC2']\n",
    "        )\n",
    "        loadings_df.attrs['expl_var'] = expl_var\n",
    "        pc1_var = expl_var[0]\n",
    "        pc2_var = expl_var[1]\n",
    "\n",
    "        for entry, (pc1, pc2) in zip(entries, pc_values):\n",
    "            entry['pc1'] = pc1\n",
    "            entry['pc2'] = pc2\n",
    "            entry['pc1_var'] = pc1_var\n",
    "            entry['pc2_var'] = pc2_var\n",
    "    else:\n",
    "        for entry in entries:\n",
    "            val = entry['vector'][0]\n",
    "            entry['pc1'] = val\n",
    "            entry['pc2'] = val\n",
    "\n",
    "    df = pd.DataFrame(entries)\n",
    "    return df.pivot(index=['sample', 'binary', 'disease'], columns='dhs', values=['pc1', 'pc2']), loadings_df\n",
    "\n",
    "\n",
    "\n",
    "config_path = \"../config.yaml\"\n",
    "dhs_files = [f.split('/')[-1].replace('.bed', '') for f in glob.glob(f\"../../raw_data/dhs/*.bed\", recursive=True)]\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "result_dir = \"../../data/moma_prostate_dhs_pca/\"\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "paper = \"\"\n",
    "metadata_path = \"../../raw_data/moma_prostate/meta_data.csv\"\n",
    "metadata_map = parse_metadata(metadata_path, paper)\n",
    "\n",
    "# for stat in STATS:\n",
    "#     print(f'Processing: {stat}')\n",
    "#     df, loadings_df = load_vectors(stat, metadata_map, result_dir, dhs_files)\n",
    "\n",
    "#     if df is not None:\n",
    "#         out_path = os.path.join(result_dir, f'feature_matrix_{stat}.parquet')\n",
    "#         df.to_parquet(out_path)\n",
    "#         print(f\"Saved: {out_path}\")\n",
    "\n",
    "#     if loadings_df is not None:\n",
    "#         loadings_out_path = os.path.join(result_dir, f'{stat}_pca_loadings.csv')\n",
    "#         loadings_df.to_csv(loadings_out_path)\n",
    "#         print(f\"Saved: {loadings_out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
