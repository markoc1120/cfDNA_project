{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import yaml\n",
    "import glob\n",
    "import torch\n",
    "import time\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, Subset\n",
    "from torchview import draw_graph\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'using device: {device}')\n",
    "\n",
    "\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "\n",
    "OUTPUT_DIR = config['output_dir']\n",
    "TRAIN_SIZE = config['train_size']\n",
    "VALID_SIZE = config['valid_size']\n",
    "BATCH_SIZE = config['batch_size']\n",
    "\n",
    "\n",
    "EPS = 1e-8\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "def sum_normalization_func(x):\n",
    "    x = x / (x.sum() + EPS)\n",
    "    return x\n",
    "\n",
    "def log_transform_func(x):\n",
    "    x = torch.log(x + EPS)\n",
    "    return x\n",
    "\n",
    "def standardization_func(x, train_mean, train_std):\n",
    "    x = (x - train_mean) / (train_std + EPS)\n",
    "    return x\n",
    "\n",
    "def slice_func(x, ymin, ymax, xmin, xmax):\n",
    "    return x[ymin:ymax,xmin:xmax]\n",
    "\n",
    "def build_pairs(matrix_dir: str):\n",
    "    \"\"\"\n",
    "    Returns list of dicts:\n",
    "    [{'sid': sid, 'positive': path, 'neg': path}, ...]\n",
    "    \"\"\"\n",
    "    npy_files = glob.glob(f'{matrix_dir}*.npy')\n",
    "\n",
    "    mapping = defaultdict(lambda: {'negative': '', 'positive': ''})\n",
    "    for p in npy_files:\n",
    "        sid = re.search(r'EE\\d+', p)[0]    \n",
    "        if not sid:\n",
    "            continue\n",
    "\n",
    "        is_neg = 'negative' in p.lower()\n",
    "        if is_neg:\n",
    "            mapping[sid]['negative'] = p\n",
    "        else:\n",
    "            mapping[sid]['positive'] = p\n",
    "\n",
    "    result = []\n",
    "    for sid, paths in mapping.items():\n",
    "        result.append({'sid': sid, 'positive': paths['positive'], 'negative': paths['negative']})\n",
    "    return result\n",
    "\n",
    "def split_pairs_torch(pairs, seed=SEED):\n",
    "    n = len(pairs)\n",
    "    train_size = n * TRAIN_SIZE // 100\n",
    "    valid_size = n * VALID_SIZE // 100\n",
    "    test_size = n - train_size - valid_size\n",
    "\n",
    "    train_pairs, valid_pairs, test_pairs = random_split(\n",
    "        pairs,\n",
    "        [train_size, valid_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "    return train_pairs, valid_pairs, test_pairs\n",
    "\n",
    "def load_train_pairs(pairs, do_sum_normalization, do_log_transform, slice_params=None):\n",
    "    matrixes = []\n",
    "    for p in pairs:\n",
    "        for path in (p['positive'], p['negative']):\n",
    "            x = torch.from_numpy(np.load(path).astype(np.float32))\n",
    "            \n",
    "            if slice_params is not None:\n",
    "                x = slice_func(x, **slice_params)\n",
    "            \n",
    "            if do_sum_normalization:\n",
    "                x = sum_normalization_func(x)\n",
    "            if do_log_transform:\n",
    "                x = log_transform_func(x)\n",
    "            matrixes.append(x)\n",
    "    X = torch.stack(matrixes, dim=0)\n",
    "    return X\n",
    "\n",
    "\n",
    "class MatrixDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pairs,\n",
    "        train_mean, \n",
    "        train_std,\n",
    "        do_standardization,\n",
    "        do_sum_normalization,\n",
    "        do_log_transform,\n",
    "        slice_params=None,\n",
    "    ):\n",
    "        self.items = []\n",
    "        for p in pairs:\n",
    "            self.items.append((p['positive'], 1, p['sid']))\n",
    "            self.items.append((p['negative'], 0, p['sid']))\n",
    "        \n",
    "        self.train_mean = train_mean\n",
    "        self.train_std = train_std\n",
    "        \n",
    "        self.do_standardization = do_standardization\n",
    "        self.do_sum_normalization = do_sum_normalization\n",
    "        self.do_log_transform = do_log_transform\n",
    "        self.slice_params = slice_params\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, y, _ = self.items[idx]\n",
    "        x = torch.from_numpy(np.load(path)).float() # [H,W]\n",
    "        \n",
    "        if self.slice_params is not None:\n",
    "            x = slice_func(x, **self.slice_params)\n",
    "        \n",
    "        if self.do_sum_normalization:\n",
    "            x = sum_normalization_func(x)\n",
    "        if self.do_log_transform:\n",
    "            x = log_transform_func(x)\n",
    "            \n",
    "        x = x.unsqueeze(0) # [1, H, W]\n",
    "        \n",
    "        if self.do_standardization and self.train_mean is not None and self.train_std is not None:\n",
    "            x = standardization_func(x, self.train_mean, self.train_std)\n",
    "            \n",
    "        return x, torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "\n",
    "def get_dataloaders(\n",
    "    do_standardization=False, \n",
    "    do_sum_normalization=False, \n",
    "    do_log_transform=False, \n",
    "    is_tiny=False, \n",
    "    slice_params=None,\n",
    "):\n",
    "    pairs = build_pairs(OUTPUT_DIR)\n",
    "    train_pairs, valid_pairs, test_pairs = split_pairs_torch(pairs, seed=SEED)\n",
    "    \n",
    "    train_mean, train_std = None, None\n",
    "    if do_standardization:\n",
    "        X = load_train_pairs(train_pairs, do_sum_normalization, do_log_transform, slice_params)\n",
    "        train_mean = X.mean().item()\n",
    "        train_std = X.std(unbiased=True).item()\n",
    "\n",
    "    DefaultMatrixDataset = partial(\n",
    "        MatrixDataset, \n",
    "        train_mean=train_mean, \n",
    "        train_std=train_std,\n",
    "        do_standardization=do_standardization,\n",
    "        do_sum_normalization=do_sum_normalization,\n",
    "        do_log_transform=do_log_transform,\n",
    "        slice_params=slice_params,\n",
    "    )\n",
    "    train_ds = DefaultMatrixDataset(train_pairs)\n",
    "    valid_ds = DefaultMatrixDataset(valid_pairs)\n",
    "    test_ds = DefaultMatrixDataset(test_pairs)\n",
    "    \n",
    "    if is_tiny:\n",
    "        train_pos_idx = [i for i, item in enumerate(train_ds.items) if item[1] == 1.0][:32]\n",
    "        train_neg_idx = [i for i, item in enumerate(train_ds.items) if item[1] == 0.0][:32]\n",
    "        train_ids = train_pos_idx + train_neg_idx\n",
    "        \n",
    "        valid_pos_idx = [i for i, item in enumerate(valid_ds.items) if item[1] == 1.0][:8]\n",
    "        valid_neg_idx = [i for i, item in enumerate(valid_ds.items) if item[1] == 0.0][:8]\n",
    "        valid_ids = valid_pos_idx + valid_neg_idx\n",
    "\n",
    "        tiny_train_ds = Subset(train_ds, train_ids)\n",
    "        tiny_valid_ds = Subset(valid_ds, valid_ids)\n",
    "        \n",
    "        tiny_train_loader = DataLoader(tiny_train_ds, batch_size=4, shuffle=True)\n",
    "        tiny_valid_loader = DataLoader(tiny_valid_ds, batch_size=4)\n",
    "        print(f'sizes:\\n {len(tiny_train_ds)} train\\n {len(tiny_valid_ds)}')\n",
    "        return tiny_train_loader, tiny_valid_loader, None\n",
    "\n",
    "    DefaultDataLoader = partial(DataLoader, batch_size=BATCH_SIZE)\n",
    "    train_loader = DefaultDataLoader(train_ds, shuffle=True)\n",
    "    valid_loader = DefaultDataLoader(valid_ds)\n",
    "    test_loader = DefaultDataLoader(test_ds)\n",
    "    \n",
    "    print(f'sizes:\\n {len(train_ds)} train\\n {len(valid_ds)} val\\n {len(test_ds)} test\\n train mean: {train_mean}\\n train std: {train_std}')\n",
    "    \n",
    "    if do_standardization:\n",
    "        total_sum, total_sumsq, total_count = 0.0, 0.0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            xb = X_batch.float() # (B, 1, H, W)\n",
    "            total_sum += xb.sum().item()\n",
    "            total_sumsq += (xb * xb).sum().item()\n",
    "            total_count += xb.numel()\n",
    "\n",
    "        sanity_mean = total_sum / total_count\n",
    "        sanity_var = total_sumsq / total_count - sanity_mean * sanity_mean\n",
    "        sanity_std = sanity_var ** 0.5\n",
    "\n",
    "        print(f'After standardization\\n mean: {sanity_mean:.2f}\\n std: {sanity_std:.2f}')\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "def evaluate_tm(model, data_loader, metric):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            metric.update(y_pred, y_batch)\n",
    "    return metric.compute()\n",
    "\n",
    "# early stopping training + save best model + scheduler\n",
    "def train(\n",
    "    model, optimizer, loss_fn, metric, train_loader, valid_loader, \n",
    "    n_epochs, patience=10, checkpoint_path=None, scheduler=None\n",
    "):\n",
    "    checkpoint_path = checkpoint_path or 'my_checkpoint.pt'\n",
    "    history = {'train_losses': [], 'train_metrics': [], 'valid_metrics': []}\n",
    "    best_metric = 0.0\n",
    "    patience_counter = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "        metric.reset()\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            metric.update(y_pred, y_batch)\n",
    "\n",
    "        train_metric = metric.compute().item()\n",
    "        valid_metric = evaluate_tm(model, valid_loader, metric).item()\n",
    "        if valid_metric > best_metric:\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            best_metric = valid_metric\n",
    "            best = ' (best)'\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            best = ''\n",
    "\n",
    "        t1 = time.time()\n",
    "        history['train_losses'].append(total_loss / len(train_loader))\n",
    "        history['train_metrics'].append(train_metric)\n",
    "        history['valid_metrics'].append(valid_metric)\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}, '\n",
    "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
    "              f\"train metric: {history['train_metrics'][-1]:.4f}, \"\n",
    "              f\"valid metric: {history['valid_metrics'][-1]:.4f}{best}\"\n",
    "              f' in {t1 - t0:.1f}s'\n",
    "        )\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(valid_metric)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    return history\n",
    "\n",
    "def build_model(m, seed=SEED):\n",
    "    torch.manual_seed(seed)\n",
    "    model = m().to(device)\n",
    "    return model\n",
    "    \n",
    "def plot_training_progress(h):\n",
    "    for plot in ('train_losses', 'valid_metrics', 'roc'):\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        for history, opt_name in zip((h.values()), h.keys()):\n",
    "            if plot == 'roc':\n",
    "                plt.plot(history['fpr'], history['tpr'], label=opt_name, linewidth=3)\n",
    "                plt.plot([0, 1], [0, 1], linestyle='--', linewidth=2, color='r', label='Random guess')\n",
    "            else:\n",
    "                plt.plot(history[plot], label=opt_name, linewidth=3)\n",
    "\n",
    "        plt.grid()\n",
    "#         plt.yscale('log')\n",
    "        plt.xlabel(\n",
    "            {\n",
    "                'train_losses': 'Epochs', \n",
    "                'valid_metrics': 'Epochs', \n",
    "                'roc': 'False positive rate'\n",
    "            }[plot]\n",
    "        )\n",
    "        plt.ylabel(\n",
    "            {\n",
    "                'train_losses': 'Training loss', \n",
    "                'valid_metrics': 'Validation AUC', \n",
    "                'roc': 'True positive rate'\n",
    "            }[plot]\n",
    "        )\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "history = defaultdict(str)\n",
    "\n",
    "def compute_best_roc_data(model, valid_loader, roc_metric):\n",
    "    model.eval()\n",
    "    roc_metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in valid_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            roc_metric.update(y_pred, y_batch.to(torch.int))\n",
    "\n",
    "    fpr, tpr, thr = roc_metric.compute()\n",
    "    return {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresholds': thr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleCNNModel:\n",
    "\n",
    "# SimpleCNNUnit (n times):\n",
    "#  - Convolution layers (2d)\n",
    "#  - LeakyReLU \n",
    "#  - MaxPool (2d)\n",
    "# AdaptiveAvgPool2d (global average pooling layer)\n",
    "# Flatten\n",
    "# Linear (head -> outputs logits)\n",
    "\n",
    "class SimpleCNNUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        DefaultConv2d = partial(nn.Conv2d, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.layers = nn.Sequential(\n",
    "            DefaultConv2d(in_channels, out_channels, stride=stride),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            DefaultConv2d(out_channels, out_channels),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "#             nn.Dropout2d(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class SimpleCNNModel(nn.Module):\n",
    "    def __init__(self, base_channels=16, dropout=0.10, num_layers=2):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            SimpleCNNUnit(1, base_channels)\n",
    "        ]\n",
    "        \n",
    "        prev_c = base_channels\n",
    "        for c in [base_channels*2, base_channels*4]:\n",
    "            layers.append(SimpleCNNUnit(prev_c, c))\n",
    "            prev_c = c\n",
    "\n",
    "        layers += [\n",
    "            nn.AdaptiveAvgPool2d(output_size=1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(prev_c, 1),\n",
    "        ]\n",
    "        self.cnn = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x).squeeze(-1)\n",
    "\n",
    "n_epochs = 20\n",
    "tag = 'cnn_1_800'\n",
    "\n",
    "model = build_model(SimpleCNNModel)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "binaryxentropy = nn.BCEWithLogitsLoss()  # outputs logits (-infinity <-> +infinity) -> use sigmoid to get probs\n",
    "binary_auc = torchmetrics.classification.BinaryAUROC().to(device)\n",
    "perf_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.1)\n",
    "\n",
    "train_loader, valid_loader, _ = get_dataloaders(slice_params={'ymin': 130, 'ymax': 200, 'xmin': 600, 'xmax':1400})\n",
    "\n",
    "history_cnn = train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    binaryxentropy,\n",
    "    binary_auc,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    n_epochs,\n",
    "    scheduler=perf_scheduler,\n",
    "    checkpoint_path=f'{tag}.pt',\n",
    ")\n",
    "\n",
    "binary_roc = torchmetrics.classification.BinaryROC().to(device)\n",
    "roc_data = compute_best_roc_data(model, valid_loader, roc_metric=binary_roc)\n",
    "\n",
    "history[tag] = {**roc_data, **history_cnn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TweakedCNNModel:\n",
    "\n",
    "class TweakedCNNUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropout=0.1, pool_ks=(2,2)):\n",
    "        super().__init__()\n",
    "        DefaultConv2d = partial(nn.Conv2d, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.layers = nn.Sequential(\n",
    "            DefaultConv2d(in_channels, out_channels, stride=stride),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            DefaultConv2d(out_channels, out_channels),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=pool_ks),\n",
    "#             nn.Dropout2d(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TweakedCNNModel(nn.Module):\n",
    "    def __init__(self, base_channels=32, dropout=0.10, num_layers=2):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            TweakedCNNUnit(1, base_channels, pool_ks=(2,1))\n",
    "        ]\n",
    "        \n",
    "        prev_c = base_channels\n",
    "        for pool_ks, c in zip([(2, 2), (1, 2)], [base_channels*2, base_channels*4]):\n",
    "            layers.append(TweakedCNNUnit(prev_c, c, pool_ks=pool_ks))\n",
    "            prev_c = c\n",
    "\n",
    "        layers += [\n",
    "            nn.AdaptiveAvgPool2d((1, 64)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(prev_c*64, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "        ]\n",
    "        self.cnn = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x).squeeze(-1)\n",
    "\n",
    "n_epochs = 20\n",
    "tag = 'cnn_1_2000_tweak'\n",
    "\n",
    "model = build_model(TweakedCNNModel)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "binaryxentropy = nn.BCEWithLogitsLoss()  # outputs logits (-infinity <-> +infinity) -> use sigmoid to get probs\n",
    "binary_auc = torchmetrics.classification.BinaryAUROC().to(device)\n",
    "perf_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.1)\n",
    "\n",
    "train_loader, valid_loader, _ = get_dataloaders(\n",
    "    # do_standardization=True,\n",
    "    # do_sum_normalization=True,\n",
    "    # do_log_transform=True,\n",
    "    is_tiny=True,\n",
    "    slice_params={'ymin': 130, 'ymax': 200, 'xmin': 600, 'xmax':1400},\n",
    ")\n",
    "\n",
    "history_cnn = train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    binaryxentropy,\n",
    "    binary_auc,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    n_epochs,\n",
    "    # scheduler=perf_scheduler,\n",
    "    checkpoint_path=f'{tag}.pt',\n",
    ")\n",
    "\n",
    "binary_roc = torchmetrics.classification.BinaryROC().to(device)\n",
    "roc_data = compute_best_roc_data(model, valid_loader, roc_metric=binary_roc)\n",
    "\n",
    "history[tag] = {**roc_data, **history_cnn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLPOnRelativeMidpointsModel (lengths are collapsed 300x2000 -> 1x2000):\n",
    "\n",
    "# SimpleMLPUnit (n times):\n",
    "#  - Linear\n",
    "#  - ReLU \n",
    "# Linear (head -> outputs logits)\n",
    "\n",
    "class SimpleMLPUnit(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class MLPOnRelativeMidpointsModel(nn.Module):\n",
    "    def __init__(self, n_inputs=2000, n_neurons=[2048, 1024, 1024, 512]):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            SimpleMLPUnit(n_in, n_out)\n",
    "            for n_in, n_out in zip([n_inputs] + n_neurons, n_neurons)\n",
    "        ] + [nn.Linear(n_neurons[-1], 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: (B, 1, 300, 2000)\n",
    "        X = X.sum(dim=2)  # (B, 1, 300, 2000) -> (B, 1, 2000)\n",
    "        X = X.squeeze(1) # (B, 1, 2000) -> (B, 2000)\n",
    "        return self.mlp(X).squeeze(1) \n",
    "\n",
    "    \n",
    "def use_he_init(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "    \n",
    "n_epochs = 100\n",
    "tag = 'mlp_1_2000'\n",
    "\n",
    "model = build_model(MLPOnRelativeMidpointsModel)\n",
    "model.apply(use_he_init)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "binaryxentropy = nn.BCEWithLogitsLoss()  # outputs logits (-infinity <-> +infinity) -> use sigmoid to get probs\n",
    "binary_auc = torchmetrics.classification.BinaryAUROC().to(device)\n",
    "perf_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.1)\n",
    "\n",
    "# train_loader, valid_loader, _ = get_dataloaders(slice_params={'ymin': 130, 'ymax': 200, 'xmin': 600, 'xmax':1400})\n",
    "train_loader, valid_loader, _ = get_dataloaders()\n",
    "\n",
    "history_relative_midpoints = train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    binaryxentropy,\n",
    "    binary_auc,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    n_epochs,\n",
    "    scheduler=perf_scheduler,\n",
    "    checkpoint_path=f'{tag}.pt',\n",
    ")\n",
    "\n",
    "binary_roc = torchmetrics.classification.BinaryROC().to(device)\n",
    "roc_data = compute_best_roc_data(model, valid_loader, roc_metric=binary_roc)\n",
    "\n",
    "history[tag] = {**roc_data, **history_relative_midpoints}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLPOnFragmentLengthsModel (relative midpoints are collapsed 300x2000 -> 300x1):\n",
    "\n",
    "# SimpleMLPUnit (n times):\n",
    "#  - Linear\n",
    "#  - ReLU \n",
    "# Linear (head -> outputs logits)\n",
    "class MLPOnFragmentLengthsModel(nn.Module):\n",
    "    def __init__(self, n_inputs=300, n_neurons=[2048, 1024, 1024, 512]):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            SimpleMLPUnit(n_in, n_out)\n",
    "            for n_in, n_out in zip([n_inputs] + n_neurons, n_neurons)\n",
    "        ] + [nn.Linear(n_neurons[-1], 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.sum(dim=3)\n",
    "        # X = fragle_transforms(X)\n",
    "        X = X.squeeze(1)\n",
    "        return self.mlp(X).squeeze(1) \n",
    "\n",
    "    \n",
    "def use_he_init(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "    \n",
    "n_epochs = 100\n",
    "tag = 'mlp_300_1'\n",
    "\n",
    "model = build_model(MLPOnFragmentLengthsModel)\n",
    "model.apply(use_he_init)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "binaryxentropy = nn.BCEWithLogitsLoss()  # outputs logits (-infinity <-> +infinity) -> use sigmoid to get probs\n",
    "binary_auc = torchmetrics.classification.BinaryAUROC().to(device)\n",
    "perf_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.1)\n",
    "\n",
    "train_loader, valid_loader, _ = get_dataloaders()\n",
    "\n",
    "history_relative_midpoints = train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    binaryxentropy,\n",
    "    binary_auc,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    n_epochs,\n",
    "    scheduler=perf_scheduler,\n",
    "    checkpoint_path=f'{tag}.pt',\n",
    ")\n",
    "\n",
    "binary_roc = torchmetrics.classification.BinaryROC().to(device)\n",
    "roc_data = compute_best_roc_data(model, valid_loader, roc_metric=binary_roc)\n",
    "\n",
    "history[tag] = {**roc_data, **history_relative_midpoints}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_progress(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = None\n",
    "for X_batch, y_batch in train_loader:\n",
    "    input_size = X_batch.shape\n",
    "    break\n",
    "    \n",
    "# model = build_model(partial(SimpleMLPUnit, in_features=2000, out_features=2048))\n",
    "model = build_model(MLPOnRelativeMidpointsModel)\n",
    "model.apply(use_he_init)\n",
    "\n",
    "model_graph = draw_graph(\n",
    "    model=model, \n",
    "    input_size=input_size, \n",
    "    device='meta', \n",
    "    expand_nested=True,\n",
    "    save_graph=True,\n",
    "    filename='mlp_relative_midpoints'\n",
    ")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total num of learnable params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
