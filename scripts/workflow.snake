#######################################
#
# Snakemake workflow analyze genomes
#
# How to run:
#
# conda create -n cfDNA_project -y snakemake snakemake-executor-plugin-slurm numpy
# conda activate cfDNA_project
# snakemake -s workflow.snake --executor slurm -j 0 --use-conda --default-resources slurm_account=Fragmentomics
#
#######################################
import glob
import yaml

with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

INPUT_DIR = config['input_dir']
INPUT_DHS_DIR = config['input_dhs_dir']
RESULT_DIR = config['result_dir']
RESULT_SORTED_DIR = config['result_sorted_dir']
MATRIX_COLUMNS = config['matrix_columns']
EXAMPLE_DHS = config['example_dhs']
RESULT_PCA_DIR = config['result_pca_dir']

SAMPLES = [f.split('/')[-1].replace('.hg38.frag.gz', '') for f in glob.glob(f"{INPUT_DIR}*.hg38.frag.gz", recursive=True)]
DHS_FILES = [f.split('/')[-1].replace('.bed', '') for f in glob.glob(f"{INPUT_DHS_DIR}*.bed", recursive=True)]
STATS = ['pfe', 'lwps', 'ifs', 'fdi', 'ocf']

rule all:
    input:
        expand(f"{RESULT_DIR}feature_matrix_{{stat}}.npy", stat=STATS),
        expand(f"{RESULT_DIR}feature_matrix_{{stat}}_meta.npz", stat=STATS),
        f"{RESULT_PCA_DIR}fdi_{EXAMPLE_DHS}_pc1_weights.png",
        f"{RESULT_PCA_DIR}fdi_{EXAMPLE_DHS}_pca_by_disease.png",
        f"{RESULT_PCA_DIR}pfe_{EXAMPLE_DHS}_violin_disease.png",
        f"{RESULT_PCA_DIR}classification_auc_by_stat.png",
        f"{RESULT_PCA_DIR}classification_pc1_auc_by_stat.png",

        
rule check_dhs_data_sorting:
    input:
        dhs=f"{INPUT_DHS_DIR}{{dhs_file}}.bed"
    output:
        dhs_sorted=f"{RESULT_SORTED_DIR}{{dhs_file}}_sorted.bed"
    shell:
        '''
        if cat {input.dhs} | awk -F'\t' '{{if(NR>1 && prev > $2 && $1 == prev_chr) {{print "PROBLEM"; exit 1}}; prev=$2; prev_chr=$1}}'; then
            # the file is sorted, only create the symlink
            ln -s $(realpath {input.dhs}) {output.dhs_sorted}
        else
            # the file isn't sorted, so we need to sort it by chr (with -V) and start position (with -n) and then gzip back
            sort -k1,1V -k2,2n {input.dhs} > {output.dhs_sorted}
        fi
        '''
        
rule preprocess_dhs_sorted_data:
    input:
        dhs_sorted=f"{RESULT_SORTED_DIR}{{dhs_file}}_sorted.bed"
    output:
        dhs_sorted_preprocessed=f"{RESULT_SORTED_DIR}{{dhs_file}}_sorted_wl{MATRIX_COLUMNS}.bed"
    script:
        "preprocess_dhs.py"
        
rule check_fragments_data_sorting:
    input:
        fragment=f"{INPUT_DIR}{{sample}}.hg38.frag.gz",
    output:
        fragment_sorted=f"{RESULT_SORTED_DIR}{{sample}}_sorted.hg38.frag.gz"
    shell:
        '''
        if zcat {input.fragment} | awk -F'\t' '{{if(NR>1 && prev > $2 && $1 == prev_chr) {{print "PROBLEM"; exit 1}}; prev=$2; prev_chr=$1}}'; then
            # the file is sorted, only create the symlink
            ln -s $(realpath {input.fragment}) {output.fragment_sorted}
        else
            # the file isn't sorted, so we need to sort it by chr (with -V) and start position (with -n) and then gzip back
            zcat {input.fragment} | sort -k1,1V -k2,2n | gzip -c > {output.fragment_sorted}
        fi
        '''

checkpoint preprocess_fragments:
    input:
        fragment=f"{RESULT_SORTED_DIR}{{sample}}_sorted.hg38.frag.gz",
        #dhs=f"{RESULT_SORTED_DIR}{{dhs_file}}_sorted.bed"
        dhs=f"{RESULT_SORTED_DIR}{{dhs_file}}_sorted_wl{MATRIX_COLUMNS}.bed"
    output:
        f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted.npy"
    script:
        "preprocess_fragments.py"
        
# LWPS calculation
rule calculate_lwps:
    input:
        matrix=f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted.npy",
        config="config.yaml"
    output:
        f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_lwps.npy"
    params:
        statistic="lwps"
    script:
        "calculate_statistics.py"

# FDI calculation
rule calculate_fdi:
    input:
        matrix=f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted.npy",
        config="config.yaml"
    output:
        f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_fdi.npz"
    params:
        statistic="fdi"
    script:
        "calculate_statistics.py"
        
# IFS calculation
rule calculate_ifs:
    input:
        matrix=f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted.npy",
        config="config.yaml"
    output:
        f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_ifs.npz"
    params:
        statistic="ifs"
    script:
        "calculate_statistics.py"

# PFE calculation
rule calculate_pfe:
    input:
        matrix=f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted.npy",
        config="config.yaml"
    output:
        f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_pfe.npz"
    params:
        statistic="pfe"
    script:
        "calculate_statistics.py"
        
# OCF calculation
rule calculate_ocf:
    input:
        matrix=f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted.npy",
        config="config.yaml"
    output:
        f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_ocf.npy"
    params:
        statistic="ocf"
    script:
        "calculate_statistics.py"
        
# merge same test statistics 
rule build_feature_matrices:
    input:
        lwps_inputs=expand(f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_lwps.npy",
                           sample=SAMPLES, dhs_file=DHS_FILES),
        ocf_inputs=expand(f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_ocf.npy",
                          sample=SAMPLES, dhs_file=DHS_FILES),
        fdi_inputs=expand(f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_fdi.npz",
                          sample=SAMPLES, dhs_file=DHS_FILES),
        ifs_inputs=expand(f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_ifs.npz",
                          sample=SAMPLES, dhs_file=DHS_FILES),
        pfe_inputs=expand(f"{RESULT_DIR}{{sample}}__{{dhs_file}}_sorted_pfe.npz",
                          sample=SAMPLES, dhs_file=DHS_FILES),      
        config="config.yaml",
    params:
        dhs_files=DHS_FILES,
    output:
        matrices=expand(f"{RESULT_DIR}feature_matrix_{{stat}}.npy", stat=STATS),
        metas=expand(f"{RESULT_DIR}feature_matrix_{{stat}}_meta.npz", stat=STATS)
    script:
        "build_matrices.py"
        
# visualize merged scores by DHS
rule pca_plots:
    input:
        matrices=expand(f"{RESULT_DIR}feature_matrix_{{stat}}.npy", stat=STATS),
        metas=expand(f"{RESULT_DIR}feature_matrix_{{stat}}_meta.npz", stat=STATS),
        config="config.yaml",
    output:
        f"{RESULT_PCA_DIR}fdi_{EXAMPLE_DHS}_pc1_weights.png",
        f"{RESULT_PCA_DIR}fdi_{EXAMPLE_DHS}_pca_by_disease.png",
        f"{RESULT_PCA_DIR}pfe_{EXAMPLE_DHS}_violin_disease.png",
    params:
        stats=STATS
    script:
        "pca.py"
        
rule classification_raw:
    input:
        matrices=expand(f"{RESULT_DIR}feature_matrix_{{stat}}.npy", stat=STATS),
        metas=expand(f"{RESULT_DIR}feature_matrix_{{stat}}_meta.npz", stat=STATS),
        config="config.yaml",
    output:
        auc_plot=f"{RESULT_PCA_DIR}classification_auc_by_stat.png"
    params:
        stats=STATS,
    script:
        "classification_svc_raw.py"
        
rule classification_pc1:
    input:
        matrices=expand(f"{RESULT_DIR}feature_matrix_{{stat}}.npy", stat=STATS),
        metas=expand(f"{RESULT_DIR}feature_matrix_{{stat}}_meta.npz", stat=STATS),
        config="config.yaml",
    output:
        auc_plot=f"{RESULT_PCA_DIR}classification_pc1_auc_by_stat.png"
    params:
        stats=STATS,
    script:
        "classification_svc_pc1.py"